{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirawitC/NLP-based-music-processing-for-composer-classification/blob/main/Musical_AI_Composer_classification_with_NLP_based_approaches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright © 2022 Somrudee Deepaisarn, Sirawit Chokphantavee, Sorawit Chokphantavee, Phuriphan Prathipasen, Suphachok Buaruk and Virach Sornlertlamvanich are authors of this computer program, contributed to the project 'Natural processing of music for composer classification' All right reserved."
      ],
      "metadata": {
        "id": "UUDQIVyYM00c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Musical AI: Composer classification with NLP-based approaches"
      ],
      "metadata": {
        "id": "3lVu7WZbYbk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "FmDPxghk4OFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries installation"
      ],
      "metadata": {
        "id": "WHnS2jd7Y88u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install three addtional package here using **PIP**, a python package installer:\n",
        "\n",
        "\n",
        "*   **pretty_midi** - the library that contain various utility function for MIDI file handling \\\n",
        "\n",
        " *   reference: [Intuitive Analysis, Creation and Manipulation of MIDI Data with pretty_midi](https://colinraffel.com/publications/ismir2014intuitive.pdf)\n",
        "*   **sentencepiece** - the unsupervised text tokenizer that allow end-to-end text tokenization without language dependency\n",
        " *   reference: [SentencePiece: A simple and language independent subword tokenizer\n",
        "and detokenizer for Neural Text Processing](https://aclanthology.org/D18-2012.pdf)\n",
        "*   **tensorflow-addons** - A library of useful extra functionality for TensorFlow\n",
        " *   reference: [TensorFlow Addons](https://www.tensorflow.org/addons)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "081XPzHYKlNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretty_midi\n",
        "!pip install sentencepiece\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "6tOV-GfmY8VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "JiAGKiczZFAN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8T6MBt3YMwp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pretty_midi\n",
        "import collections\n",
        "from typing import Dict, List, Optional, Sequence, Tuple\n",
        "import math\n",
        "import string\n",
        "import gensim\n",
        "import ast\n",
        "import sentencepiece as spm\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "TN6dNO5NZO00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will mount our colab notebook to our Google Drive to access all the data."
      ],
      "metadata": {
        "id": "Smp1NHVbNsGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "yZRcqlIkZPJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we specify our `music_path_prefix` which will be used later in this notebook."
      ],
      "metadata": {
        "id": "71lonGRHOPQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "music_path_prefix = \"/content/drive/MyDrive/Musical AI/Dataset/maestro-v3.0.0/\""
      ],
      "metadata": {
        "id": "Xbwug7t8OWTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will access the **maestroV3.csv**, which contain all information in MAESTRO Dataset ver.3, using `read_csv` function from pandas library. \n",
        "\n",
        "*   **maestro_data** - contain all information from MAESTRO Dataset in `pandas.DataFrame` format\n",
        "*  **composer_100** - contain all information of 5 composers of interest including Franz Liszt, Franz Schubert, Frédéric Chopin, Johann Sebastian Bach, andLudwig van Beethoven.\n",
        "\n"
      ],
      "metadata": {
        "id": "5ny_UGEpO10u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maestro_data = pd.read_csv(\"/content/drive/MyDrive/Musical AI/Dataset/maestroV3.csv\")\n",
        "composer_100 = maestro_data[maestro_data[\"canonical_composer\"].isin([\"Franz Liszt\",\"Franz Schubert\",\"Frédéric Chopin\",\"Johann Sebastian Bach\",\"Ludwig van Beethoven\"])]"
      ],
      "metadata": {
        "id": "U4txFjpbZdHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_gram_list = np.load(\"/content/drive/MyDrive/Musical AI/Dataset/all_gram_list.npy\")\n",
        "# all_gram_list_vel = np.load(\"/content/drive/MyDrive/Musical AI/Dataset/all_gram_list_vel.npy\")"
      ],
      "metadata": {
        "id": "z5FmAAuPOYZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create utility functions"
      ],
      "metadata": {
        "id": "QgdxFX8uaQjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**midi_to_notes** - this function allow us to extract the useful music       characteristic such as \"pitch\", \"note start\", \"note end\", and \"velocity\" from music in MIDI format.\n",
        "*   Input - file path of the interested MIDI file\n",
        "*   Output - `pandas.DataFrame` contain the musical features (pitch, start, end, step, duration, velocity)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ikpehnvPQuEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def midi_to_notes(midi_file: str) -> pd.DataFrame:\n",
        "  pm = pretty_midi.PrettyMIDI(midi_file)\n",
        "  instrument = pm.instruments[0]\n",
        "  notes = collections.defaultdict(list)\n",
        "\n",
        "  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)\n",
        "  prev_start = sorted_notes[0].start\n",
        "\n",
        "  for note in sorted_notes:\n",
        "    start = note.start\n",
        "    end = note.end\n",
        "    notes['pitch'].append(note.pitch)\n",
        "    notes['start'].append(start)\n",
        "    notes['end'].append(end)\n",
        "    notes['step'].append(start - prev_start)\n",
        "    notes['duration'].append(end - start)\n",
        "    notes['velocity'].append(note.velocity)\n",
        "    prev_start = start\n",
        "  return pd.DataFrame({name: np.array(value) for name, value in notes.items()})"
      ],
      "metadata": {
        "id": "oLlQPquOQtTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**extract_gram** - this function will extract the MIDI information of each note into tuple `(Pi, Di)` \\\n",
        "> where : \\\n",
        " Pi is the pitch number of each specific note\\\n",
        " Di is the duration that each note played\n",
        "\n",
        "*   Input - `midi_frame` which is the DataFrame contain each music piece information\n",
        "*   Output - data in form of `list` containing all extracted tuple of every music\n",
        "\n"
      ],
      "metadata": {
        "id": "IzSyks5vTKYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_gram(midi_frame):\n",
        "  gram_list = []\n",
        "  temp = []\n",
        "  s_time = 0\n",
        "  for i in range(midi_frame.shape[0]):\n",
        "    pitch = midi_frame[\"pitch\"][i]\n",
        "    ti = round(midi_frame[\"duration\"][i],2)\n",
        "    gram = (pitch,ti)\n",
        "    \n",
        "    if((not temp) or (midi_frame[\"start\"][i] - s_time <= 0.003)):\n",
        "      temp.append(gram)\n",
        "      if(len(temp) == 1):\n",
        "        s_time = midi_frame[\"start\"][i]\n",
        "      if(i == midi_frame.shape[0] - 1):\n",
        "        gram_list += temp\n",
        "    else:\n",
        "      sorted_list = sorted(temp, key=lambda tup: tup[0], reverse=True)\n",
        "      sorted_list.append(gram)\n",
        "      gram_list += sorted_list\n",
        "      temp.clear()\n",
        "      s_time = 0\n",
        "\n",
        "  return gram_list"
      ],
      "metadata": {
        "id": "GwgpSwmbTJuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**encodeChinese** - this function will encode integer (`int`) into chinese uni-code character.\n",
        "*   Input - any integer number\n",
        "*   Output - chinese uni-code character\n",
        "\n"
      ],
      "metadata": {
        "id": "CbTS8apQTXRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encodeChinese(index_number):\n",
        "  val = index_number + 0x4e00\n",
        "  return chr(val)"
      ],
      "metadata": {
        "id": "RFq_XGcYTWLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**get_sentence_vec_avg** - this function will transform the tokenized word/subword list into average vector representation\n",
        "*   Input - list of tokenized word/subword and `Word2Vec` model\n",
        "*   Output - vector representation of each music\n",
        "\n"
      ],
      "metadata": {
        "id": "Ac7ZxTHvpMKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_vec_avg(sentences,model):\n",
        "  l = []\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      try:\n",
        "        temp = np.zeros(len(model[word]))\n",
        "        temp += model[word]\n",
        "      except:\n",
        "        print(\"Not in vocab\")\n",
        "    l.append(temp/len(sentence))\n",
        "  return l"
      ],
      "metadata": {
        "id": "rykxTPZypKKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**get_sentence_vec_avg_with_cov2** - this function will transform the tokenized word/subword list into average vector concatenated with SD vector representation\n",
        "*   Input - list of tokenized word/subword and `Word2Vec` model\n",
        "*   Output - vector representation of each music\n",
        "\n"
      ],
      "metadata": {
        "id": "TbSTqA7cro6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_vec_avg_with_cov2(sentences,model):\n",
        "  l = []\n",
        "  cov = []\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      try:\n",
        "        temp = np.zeros(len(model[word]))\n",
        "        temp += model[word]\n",
        "        cov.append(model[word])\n",
        "      except:\n",
        "        print(\"Not in vocab\")\n",
        "    data = np.array(cov)\n",
        "    sd = np.std(data,axis=0)\n",
        "    z = temp/len(sentence)\n",
        "    z = z.tolist()\n",
        "    z += sd.tolist()\n",
        "    z = np.array(z)\n",
        "    l.append(z)\n",
        "  return l"
      ],
      "metadata": {
        "id": "3_73hMF-rd73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**get_sentence_vec_SD_only** - this function will transform the tokenized word/subword list into SD vector representation\n",
        "*   Input - list of tokenized word/subword and `Word2Vec` model\n",
        "*   Output - vector representation of each music\n",
        "\n"
      ],
      "metadata": {
        "id": "O_mLk4_Ar2ZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_vec_SD_only(sentences,model):\n",
        "  l = []\n",
        "  cov = []\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      try:\n",
        "        cov.append(model[word])\n",
        "      except:\n",
        "        print(\"Not in vocab\")\n",
        "    data = np.array(cov)\n",
        "    sd = np.std(data,axis=0)\n",
        "    z = sd.tolist()\n",
        "    z = np.array(z)\n",
        "    l.append(z)\n",
        "  return l"
      ],
      "metadata": {
        "id": "L36AgpA8r0oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**createLabel** - this function will create list of associated composer label for each  music piece represented in vector\n",
        "*   Input - list of every vector representation of music \n",
        "*   Output - data of music vector and its associated classification label\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7bCiarTrr9pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createLabel(sentenceslst):\n",
        "  composer_music = maestro_data.groupby([\"canonical_composer\"])[\"canonical_title\"].count().to_frame()\n",
        "  composer_music.columns = [\"canonical_title\"]\n",
        "  composer_music.reset_index(inplace=True)\n",
        "  composer_list= []\n",
        "  for i in range(composer_music.shape[0]):\n",
        "    composer_list.append(composer_music.iloc[i][0])\n",
        "  composer_map = { j:i for i,j in enumerate(composer_list)}\n",
        "  data = []\n",
        "  label = []\n",
        "  for i in composer_100.index.tolist():\n",
        "    try:\n",
        "      data.append(sentenceslst[i])\n",
        "      label.append(composer_map[maestro_data.iloc[i][\"canonical_composer\"]])\n",
        "    except:\n",
        "      print(\"Error\",i)\n",
        "  return data, label"
      ],
      "metadata": {
        "id": "c7WG7jlIr9QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**get_f1** - this function will calculate the F1-Score that will be an evaluation metric for the MLP model.\n",
        "\n",
        "\n",
        "*   Inputs - ground truth labels, prediction results \n",
        "*   Output - F1-Score (ranging from 0.00 - 1.00)\n",
        "\n"
      ],
      "metadata": {
        "id": "LLyhi8Hi4-Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "metadata": {
        "id": "lt6IkoTKbCw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "oaBGSo1m4lc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### variable initialization"
      ],
      "metadata": {
        "id": "vo_B5M9Uaf-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract all tuples `(Pi, Di)` from all songs in the dataset and record them into a list called **\"all_gram_list\"**."
      ],
      "metadata": {
        "id": "X4aq49Ekxlf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_gram_list = []\n",
        "for i in range(maestro_data.shape[0]):\n",
        "  suffix_path = maestro_data[\"midi_filename\"][i]\n",
        "  path = music_path_prefix + suffix_path\n",
        "  frame = midi_to_notes(path)\n",
        "  gram_list = extract_gram(frame)\n",
        "  all_gram_list = all_gram_list + gram_list"
      ],
      "metadata": {
        "id": "sjihZRrdalqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, save the **\"all_gram_list\"** as a `numpy.array` for convenience usage later."
      ],
      "metadata": {
        "id": "oi-HQLnHyqoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/drive/MyDrive/Musical AI/Dataset/all_gram_list_note_as_char2.npy\",all_gram_list)"
      ],
      "metadata": {
        "id": "_nmAGyS6ao5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the saved `numpy.array` into the variable called **\"all_gram_list\"**."
      ],
      "metadata": {
        "id": "4JZpmJdtzV9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_gram_list = np.load(\"/content/drive/MyDrive/Musical AI/Dataset/all_gram_list_note_as_char2.npy\")"
      ],
      "metadata": {
        "id": "GHD7Ixbgaqrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity check: try printing out the partial result of **\"all_gram_list\"** that we loaded."
      ],
      "metadata": {
        "id": "wyM7gOdkzq0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_gram_list[0:20]"
      ],
      "metadata": {
        "id": "AACJOnUCasp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we sort the **\"all_gram_list\"**, then use that sorted list to create a dictionary for mapping the note tuple into a Chinese character and vice versa. "
      ],
      "metadata": {
        "id": "CByzuC7f0P0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_gram_list = sorted(set(tuple(i) for i in all_gram_list.tolist()))\n",
        "note2Ch = { j:encodeChinese(i) for i,j in enumerate(sorted_gram_list)}\n",
        "Ch2note =  { encodeChinese(i):j for i,j in enumerate(sorted_gram_list)}"
      ],
      "metadata": {
        "id": "tezifJtfauaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build corpus"
      ],
      "metadata": {
        "id": "vuOsjfCVaw_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**extractInfoToTxt** - this function will extract the note tuples, map them into Chinese characters, and record them into a text file.\n",
        "\n",
        "\n",
        "*   Input - name of the file\n",
        "*   Output - a file with the specified name in `.txt` format\n",
        "\n"
      ],
      "metadata": {
        "id": "t5QFJbtx5PLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extractInfoToTxt(filename):\n",
        "  text = ''\n",
        "  for i in range(maestro_data.shape[0]):\n",
        "    suffix_path = maestro_data[\"midi_filename\"][i]\n",
        "    path = music_path_prefix + suffix_path\n",
        "    frame = midi_to_notes(path)\n",
        "    gram_list = extract_gram(frame)\n",
        "    for j in gram_list:\n",
        "      text += note2Ch[j]\n",
        "    text += '\\n'\n",
        "  f = open(filename, \"w\")\n",
        "  f.write(text)\n",
        "  f.close() "
      ],
      "metadata": {
        "id": "dPSxN8MNawj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilize the function to build the text corpus."
      ],
      "metadata": {
        "id": "l2hF4J176ovE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractInfoToTxt(\"CorpusNoteAsChar2.txt\")"
      ],
      "metadata": {
        "id": "nP_VG2K8a3V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "Pb-qReDIEn3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SentencePiece"
      ],
      "metadata": {
        "id": "IY_s1mJha4lp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sentencePiece** - this function will be used to build a subword tokenizer model according to the NLP-based approach, SentencePiece [Taku Kudo & John Richardson](https://arxiv.org/abs/1808.06226). Then use the model for tokenizing the corpus.\n",
        "\n",
        "\n",
        "*   Inputs - file path of the corpus, preferred name of the model, size of the vocabulary, and maximum sentence length \n",
        "*   Output - List of all tokenized sentences (songs)\n",
        "\n"
      ],
      "metadata": {
        "id": "EniH_lim2Wmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentencePiece(corpus, modelName, vocabSize, maxSenLength):\n",
        "  spm.SentencePieceTrainer.train(input=corpus, model_prefix=modelName, vocab_size=vocabSize, max_sentence_length=maxSenLength)\n",
        "  sp = spm.SentencePieceProcessor()\n",
        "  temp = modelName+\".model\"\n",
        "  sp.load(temp)\n",
        "  f1 = open(corpus,'r')\n",
        "  temp = {}\n",
        "  for i in range(1276):\n",
        "    line = f1.readline()\n",
        "    tokenized = sp.encode_as_pieces(line)\n",
        "    temp[i] = tokenized\n",
        "  Ch_note_series = pd.Series(temp)\n",
        "  f1.close()\n",
        "  sentences = []\n",
        "  for i in range(Ch_note_series.shape[0]):\n",
        "    sentences.append(Ch_note_series[i])\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "txftlkjea7MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "MZbeSdBjbEqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec** - this function will transform each tokenized subword in each sentence into its representation vectors utilizing the Word2Vec procedure, as proposed by [Mikolov et al.](https://arxiv.org/pdf/1301.3781.pdf). Note that here we use the Word2Vec approach with the skip-gram algorithm. \n",
        "\n",
        "\n",
        "*   Inputs - Skip-gram window size, list of all tokenized sentences, Average (boolean), Standard deviation (boolean) \n",
        "\n",
        "\n",
        "All valid combinations of Avg and SD:\n",
        "> `Avg = True, SD = False` : each song is the average vector of all its subword vectors. \\\n",
        "`Avg = False, SD = True`: each song is the SD vector of all its subword vectors. \\\n",
        "`Avg = True, SD = True` : each song is the average vector concatenate with SD vector \n",
        "\n",
        "*   Output - List of all song represented vector.\n",
        "\n"
      ],
      "metadata": {
        "id": "1jVGCC4d6gWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Word2Vec(Window, sentences, Avg = False, SD = False):\n",
        "  model = gensim.models.Word2Vec(\n",
        "    sentences=sentences,\n",
        "    window=Window,\n",
        "    min_count=1,\n",
        "    workers=4,\n",
        "    sg = 1\n",
        "  )\n",
        "  if(Avg and SD):\n",
        "      sentenceLstAvgwithCov = get_sentence_vec_avg_with_cov2(sentences,model)\n",
        "      return sentenceLstAvgwithCov\n",
        "  elif(Avg and (not SD)):\n",
        "      sentencesLstAvg = get_sentence_vec_avg(sentences,model)\n",
        "      return sentencesLstAvg\n",
        "  elif((not Avg) and SD):\n",
        "      sentencesLstSD = get_sentence_vec_SD_only(sentences,model)\n",
        "      return sentencesLstSD"
      ],
      "metadata": {
        "id": "jFi9f9zlbJAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "Khnxk085E8pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main function"
      ],
      "metadata": {
        "id": "1cYyeeOQbKgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main** - this function integrate subfunctions and utility functions so that it allow us to conduct experiment easily with varying parameters. The classification algorithm used in this function are K-Nearest Neighbor (KNN), Random Forest Classifier (RFC), Logistic Regression (LR), Support Vector Machine (SVM), and Multilayer Perceptron (MLP)\n",
        "\n",
        "\n",
        "*   Inputs - corpus's file path, model name (default is \"m\"), vocab size for Word2Vec algorithm, max sentence length for Word2Vec algorithm, skip-gram's window size, Avg, and cov combination as describe in Word2Vec section \n",
        "*   Outputs - test F1-score result from KNN, RFC, LR, SVM, MLP and Avg parameter\n",
        "\n"
      ],
      "metadata": {
        "id": "iqJO21EEAXHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Main(corpus, modelName, vocabSize, maxSenLength, window, Avg=False, cov=False):\n",
        "  sentences = sentencePiece(corpus, modelName, vocabSize, maxSenLength)\n",
        "  sentencesLst = Word2Vec(window, sentences, Avg, cov)\n",
        "  data, label = createLabel(sentencesLst)\n",
        "  sentence_w_label_100 = pd.DataFrame({\"sentence\": data, \"label\":label})\n",
        "  map_label = {11:0, 14:1, 18:2, 31:3, 40:4}\n",
        "  # Create training data\n",
        "  X = []\n",
        "  y = []\n",
        "  for i in composer_100.index.tolist():\n",
        "      X.append(sentencesLst[i])\n",
        "  for i in sentence_w_label_100[\"label\"].tolist():\n",
        "    y.append(map_label[i])\n",
        "  PredictorScaler=StandardScaler()\n",
        "  PredictorScalerFit=PredictorScaler.fit(X)\n",
        "  X=PredictorScalerFit.transform(X)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "  y_train_mlp = tf.one_hot(y_train, 5)\n",
        "  y_test_mlp = tf.one_hot(y_test, 5)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "  #KNN\n",
        "  clf = KNeighborsClassifier(n_neighbors=5)\n",
        "  KNN=clf.fit(X_train,y_train)\n",
        "  prediction=KNN.predict(X_test)\n",
        "  F1_Score_knn=metrics.f1_score(y_test, prediction, average='weighted')\n",
        "  #Random forest\n",
        "  clf = RandomForestClassifier(max_depth=500, random_state=0)\n",
        "  RFC = clf.fit(X_train, y_train)\n",
        "  prediction=RFC.predict(X_test)\n",
        "  F1_Score_RFC=metrics.f1_score(y_test, prediction, average='weighted')\n",
        "  #Logistic regression\n",
        "  log = LogisticRegression(random_state=0, max_iter=300).fit(X_train, y_train)\n",
        "  prediction=log.predict(X_test)\n",
        "  F1_Score_Logistic=metrics.f1_score(y_test, prediction, average='weighted')\n",
        "  #SVM\n",
        "  svm = SVC(random_state=0).fit(X_train, y_train)\n",
        "  predictionSVM = svm.predict(X_test)\n",
        "  F1_Score_SVM=metrics.f1_score(y_test, predictionSVM, average='weighted')\n",
        "  #MLP\n",
        "  array_shape = len(X[0])\n",
        "  model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Input(shape=(array_shape,)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(256,activation='tanh',\n",
        "    kernel_regularizer = tf.keras.regularizers.l2(1e-4)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(128,activation='tanh',\n",
        "    kernel_regularizer = tf.keras.regularizers.l2(1e-4)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(5,activation='softmax')\n",
        "  ])\n",
        "  model.compile(optimizer='adam', \n",
        "             loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "             metrics=[get_f1])\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "             filepath = '/content/checkpoint',\n",
        "             save_weights_only = True,\n",
        "             monitor = 'val_get_f1',\n",
        "             mode = 'max',\n",
        "             save_best_only = True\n",
        "  )\n",
        "  history = model.fit(X_train, y_train_mlp, epochs=200, batch_size=256,\n",
        "                    shuffle=True, validation_split=0.2, callbacks=[model_checkpoint_callback])\n",
        "  model.load_weights('/content/checkpoint')\n",
        "  test_loss, F1_Score_MLP = model.evaluate(X_test,y_test_mlp,verbose=0)\n",
        "  return F1_Score_knn, F1_Score_RFC, F1_Score_Logistic, Avg, F1_Score_SVM, F1_Score_MLP"
      ],
      "metadata": {
        "id": "2dBpFHZObNuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation example"
      ],
      "metadata": {
        "id": "0Ywuj-55bRoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AVG"
      ],
      "metadata": {
        "id": "iAuKERwJblv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN, RFC, Logis, Avg, SVM, MLP = Main(\"CorpusNoteAsChar2.txt\",\"m\",13000,5000,5,True,False)\n",
        "print(\"Avg:\", Avg)\n",
        "print(\"F1-score of KNN:\",KNN)\n",
        "print(\"F1-score of RFC:\",RFC)\n",
        "print(\"F1-score of Logistic:\",Logis)\n",
        "print(\"F1-score of SVM:\",SVM)\n",
        "print(\"F1-score of MLP:\",MLP)"
      ],
      "metadata": {
        "id": "dwd66HiWbUEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SD"
      ],
      "metadata": {
        "id": "wfZNYwvnbmvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN, RFC, Logis, Avg, SVM, MLP = Main(\"CorpusNoteAsChar2.txt\",\"m\",13000,5000,5,False,True)\n",
        "print(\"Avg:\", Avg)\n",
        "print(\"F1-score of KNN:\",KNN)\n",
        "print(\"F1-score of RFC:\",RFC)\n",
        "print(\"F1-score of Logistic:\",Logis)\n",
        "print(\"F1-score of SVM:\",SVM)\n",
        "print(\"F1-score of MLP:\",MLP)"
      ],
      "metadata": {
        "id": "Y-uL6nXTbeb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AVG + SD"
      ],
      "metadata": {
        "id": "nbjyf-rjbpbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN, RFC, Logis, Avg, SVM, MLP = Main(\"CorpusNoteAsChar2.txt\",\"m\",13000,5000,5,True,True)\n",
        "print(\"Avg:\", Avg)\n",
        "print(\"F1-score of KNN:\",KNN)\n",
        "print(\"F1-score of RFC:\",RFC)\n",
        "print(\"F1-score of Logistic:\",Logis)\n",
        "print(\"F1-score of SVM:\",SVM)\n",
        "print(\"F1-score of MLP:\",MLP)"
      ],
      "metadata": {
        "id": "vcCVLoe0bsHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright © 2022 Somrudee Deepaisarn, Sirawit Chokphantavee, Sorawit Chokphantavee, Phuriphan Prathipasen, Suphachok Buaruk and Virach Sornlertlamvanich are authors of this computer program, contributed to the project 'Natural processing of music for composer classification' All right reserved."
      ],
      "metadata": {
        "id": "N8eCgmt4NHJi"
      }
    }
  ]
}